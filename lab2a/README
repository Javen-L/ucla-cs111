### DELIVERABLES
The submission tarball contains the required 19 files:

1. Makefile
    It has 5 main targets: build, tests, graphs, dist, and clean.
    'build' is used to build executable lab2_add and lab2_list from corresponding source codes.
    'tests' is used to create csv files. It has subtargets test_add and test_list, 
            each of which contains test cases for each program.
    'graphs' is used to produce png files using lab2_add.pg and lab2_list.pg provided.
    'dist' is used to create a submission tarball. It has subtarget build, tests, and graphs.
    
2. README
    This file.

3. lab2_add.c
    A source code for lab2_add. It has options --iterations, --threads, --sync=[msc], and --yield.
    By default, the program sets --iterations=1 and --threads=1.

4. lab2_list.c
    A source code for lab2_list. It has options --iterations, --threads, --sync=[ms], and --yield.
    By default, the program sets --iterations=1 and --threads=1.    

5, 6. SortedList.c and SortedList.h
    SortedList.c contains implementation for SortedList functions. Each methods is implemented as specified in SortedList.h.

7-15. png files generated by 'make graphs' command. Descriptions for each file are as follows (copied from the spec):
    lab2_add-1.png ... threads and iterations required to generate a failure (with and without yields)
    lab2_add-2.png ... average time per operation with and without yields.
    lab2_add-3.png ... average time per (single threaded) operation vs. the number of iterations.
    lab2_add-4.png ... threads and iterations that can run successfully with yields under each of the synchronization options.
    lab2_add-5.png ... average time per (protected) operation vs. the number of threads.
    lab2_list-1.png ... average time per (single threaded) unprotected operation vs. number of iterations.
    lab2_list-2.png ... threads and iterations required to generate a failure (with and without yields).
    lab2_list-3.png ... iterations that can run (protected) without failure.
    lab2_list-4.png ... (length-adjusted) cost per operation vs the number of threads for the various synchronization options.

16, 17. lab2_list.csv and lab2_list.csv
    CSV files generated by 'make tests' command. Results of failed tests are not recorded in those files.

18, 19. lab2_add.gp and lab2_list.gp
    Scripts for generating graphs (i.e. png files).


### ANSWERS TO QUESTIONS
QUESTION 2.1.1 - causing conflicts:
    It takes many iterations before errors are seen because the more iterations we have, 
    the more chances for each thread accesses/modifies the shared counter.
    So, if the number of iterations is small, there are less chances to fail.

QUESTION 2.1.2 - cost of yielding:
    The program runs slower with --yield option becasuse of context switch. Each time 
    sched_yield() is called, the current thread is blocked and placed in the end of the queue.
    This process (context switch) requires an extra time, hence the longer execution time.

    It is not possible to get a precise time per operation because sched_yield() is placed in the
    middle of an operation, and causing a context switch. So, a whole run time will containsthose
    time used for context switches, thus, total time divided by number of operations is not equal
    to the per-oeration time.

QUESTION 2.1.3 - measurement errors:
    The average cost per operation drops with increasing iterations becasue time to create a
    thread is also included in the total time. So, our time per operation (=T') is acureally
    time per operation (=T) plus time for creating a thread (=C) divided by the number of
    operations (=2N, where N is the number of iterations). That is,
        T' = T + C/2N --(1)
    Theoretically, T does not change depending on N. So, if we increase N, C/2N decreases,
    hence T' increases.

    From (1), we can see that the correct cost per operation can be approximately calculated
    by choosing a large N such that C/2N~0, hence T'~T.

QUESTION 2.1.4 - costs of serialization:
    All options perform similarly when the number of threads is small because each protection is
    used less often than it is when the number of threads is large. In an extreme case where
    we run the program with just one thread, all cases have almost identical cost per operations
    because each protection is used only for checking if the current thread should wait or not
    (e.g. we may execute one line '__sync_lock_test_and_set(&lock_s, 1)' but do not execute the
    loop like we would do with multiple threads).

    When the number of threads rises, each protection are used more frequently. That is,
    there are more chances that multiple threads execute a critical sextion at the same time.
    Obviously, each protection is not free, so it consumes some time when used. So, if we
    have more threads, more chances that those protections are used, resulting in more 
    expensive cost per operation.

QUESTION 2.2.1 - scalability of Mutex
    Time per mutex-protected operation (=C) vs the number of threads (=T) in part1 (lab2_add-5.png)
    shows that C is approximately propotional to T. That is, as T increases, C increases as well.
    So, the shape of the curve looks like a linear line C = a*T for some constant a.
    This is for the reasons explained in Q2.1.4.

    However, in part 2, C does not seem to change based on the value of T. Moreover, the shape of the
    curve almost looks like a constant line (i.e. C ~ b for some constant b).
    This can be explained by the way I used the lock. Namely, I locked the whole list (as suggested by
    a TA on piazza @479) instead of locking each step of insert, delete, lookup, and get length.
    This resulted in thread_routine being executed basically by one thread at a time without any
    additional operations (such as yield, lock-unlock). So, C has almost nothing to do with T in
    this case, and thus the curve looks like almost a constant line.

QUESTION 2.2.2 - scalability of spin locks
    Time per mutex-protected operation (=Cm) vs the number of threads (=T) has the relationship
    as described in Q2.2.1. On the other hand, time per spin-protected operation (=Cs) increases
    as T increases. This is similar to the relationship between mutex and spin locks seem in part
    1. Namely, in part 1, Cs increases fastor than Cm as T increases. This can be explained due to
    the fact that a spin lock uses a loop to keep checking if the lock is releases or not, which
    can be added up tp the total run time. If total run time increases, naturally time per operation
    increases becasue we are computing it by simply dividing total time by a theoretical number of
    operations that does not include the loop operations done by spin lock.


### REFERENCES
https://pubs.opengroup.org/onlinepubs/9699919799/functions/pthread_join.html
https://www.geeksforgeeks.org/mutex-lock-for-linux-thread-synchronization/
https://en.wikipedia.org/wiki/Spinlock
https://attractivechaos.wordpress.com/2011/10/06/multi-threaded-programming-efficiency-of-locking/
https://en.wikipedia.org/wiki/Compare-and-swap
