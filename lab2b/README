### DELIVERABLES
The submission tarball contains the required 13 files:

1. Makefile
    It has 6 main targets: default, tests, profile, graphs, dist, and clean.
    'default' is used to build executable lab2_list from corresponding source codes.
    'tests' is used to create lab2b_list.csv. It has subtargets test0-3.
    'profile' is used to create profile.out. It uses gperftools.
    'graphs' is used to produce png files using lab2_list.pg.
    'dist' is used to create a submission tarball. It has subtarget default and tests.
    
2. README
    This file.

3. lab2_list.c
    A source code for lab2_list. It has options --iterations, --threads, --sync=[ms], --yield,
    and --lists. By default, the program sets --iterations=1, --threads=1, and --lists=1.
    It is mostly the same as lab2_list.c.    

4, 5. SortedList.c and SortedList.h
    SortedList.c contains implementation for SortedList functions.
    Each methods is implemented as specified in SortedList.h.

6-10. Graphs generated by 'make graphs' command. Descriptions are as follows (copied from the spec):
    lab2b_1.png ... throughput vs. number of threads for mutex and spin-lock list operations.
    lab2b_2.png ... avg time per mutex wait and avg time per operation for mutex list operations.
    lab2b_3.png ... successful iterations vs. threads for each synchronization method.
    lab2b_4.png ... throughput vs. number of threads for mutex synchronized partitioned lists.
    lab2b_5.png ... throughput vs. number of threads for spin-lock-synchronized partitioned lists.

11. lab2b_list.csv
    CSV file generated by 'make tests' command. Results of failed tests are not recorded in
    those files.

12. lab2_list.gp
    Scripts for generating graphs (i.e. png files).

13. profile.out
    A file containing the output of 'make profile' command.


### ANSWERS TO QUESTIONS
QUESTION 2.3.1 - Cycles in the basic list implementation:
    When using one or two threads, I believe the program spends most of the time doing
    list operations. This is becasue the time for getting locks can be very short with
    a small number of threads. Specifically, if we are using only one thread, the time
    spent to get the lock should be (theoretically) 0 because it's always unlocked.

    When using large numner of threads with spin-lock, I believe the program spends
    most of the time where __sync_lock_test_and_set() is used. This is because there
    are more chances that the lock is kept by other threads than when the program is
    run with a small number of threads.

    When using large number of threads with mutex, I beleive the program spends most
    of the time being blocked due to the fact that the other thread is holding the lock.
    Similar to spin-lock, chances of getting blocked increases as number of threads
    increases.

QUESTION 2.3.2 - Execution Profiling:
    By looking at profile.out, it is clear that the program spends most of the time in
    thread_routine(). Moreover, gperftools's --list option allows us to comfirm that
    the program spends majority of the time at __sync_lock_test_and_set(), as expected.

    This operation becomes more expensive with large number of threads because of an
    increased lock contentions. That is, the more threads there are, the less likely
    a thread can acquire the lock, resulting a longer wait time. 

QUESTION 2.3.3 - Mutex Wait Time:
    The average wait-for-lock time increases dramatically with the number of threads
    becasue there are more chances that the lock is kept by other threads.

    Time per operation increases with the number of threads as well because these are
    more context switches. However, in theory, time spent doing a list operation should
    be independent of the number of threads. Thus, time per operation increases as the
    number of threads increases, but less dramatically than the wait-lock-time per
    operation.

QUESTION 2.3.4 - Performance of Partitioned Lists    
    From the graphs, we can see that throughput is proportional to the number of lists.
    So, as the number of lists increases, throughput increases as well. This is becasue
    using more lists allows more threads to run at the same time.

    Throughput should continue increasing as the number of lists increases up to a certain
    point, after which it decreases. This is becasue if we partition the list into smaller
    pieces (a single node in the extreme case), time spent locking and unlocking for each
    small piece would be excessively high (textbook p344).

    From lab2b_4.png, for example, the throughput of 4-way partitioned list with 16 threads
    is about 3e+5, whereas the throughput of a single list with 4 threads is about 1+e5.
    Also, from lab2b_5.png, the throughput of 4-way partitioned list with 16 threads
    is about 45e+4, whereas the throughput of a single list with 4 threads is about 13+e4.
    So it appears that the statement is not true. This can be due to many reasons. For
    example, the partitioned lists are not necessarily of the same size, or time spent
    locking/unlocking could affect throughtput.


### REFERENCES
http://euccas.github.io/blog/20170827/cpu-profiling-tools-on-linux.html
https://github.com/rishabkdoshi/CS111/blob/master/week6/pprof-example.md
https://gperftools.github.io/gperftools/cpuprofile.html